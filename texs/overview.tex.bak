%In this work, we have provided an understanding of IRL in the KWIK framework which has not been studied before. Apart from the practical relevance of a KWIK imitation learning algorithm, this setup provides scope for theoretical guarantees that can be provided for active query based IRL which have not been provided so far. We have also laid the groundwork for a new class of algorithms that can be termed as `KWIK classifiers' that will suit tasks similar to IRL-based imitation learning. Since the existing selective sampling based classification approaches do not apply here due to their strong assumptions, an appropriate direction for future work will be to design admissible KWIK classification algorithms that suit our conditions.


In this work we have dealt  with understanding the problem of imitation learning in the active learning setting. We have specifically chosen inverse reinforcement learning as a means for solving imitation learning and explored how this can be performed by an active learner. 

Furthermore, we have consider this problem in an online \textit{interactive} framework where the learner is allowed to make queries to the expert only on states that it encounters. We have identified the Knows-What-It-Knows framework to be suited for this purpose and also examined the practical relevance of considering such a learner: the burden on the teacher is reduced, the learner knows when to query, and whenever the learner takes a decision autonomously the learner is near-optimal. Besides the practical importance of studying the problem in the KWIK framework, we also see that it helps us provide theoretical guarantees which have not been studied before in the (inter-)active learning algorithms for cost-based imitation learning methods (i.e., inverse reinforcement learning) like \citet{Chernova:2009:IPL:1622716.1622717} and \citet{DBLP:conf/icra/SilverBS12}.

To make use of the MDP structure and similarity across state-action pairs we modeled the problem as an MDP with linear parametrization of the costs of decisions. To reduce the problem to a KWIK learning problem, we looked at the queries as queries about comparisons between pairs of parametrized actions. We showed how this reduces to classification, and the problem of finding the weights given to various features of these costs boils down to finding a separating hyperplane passing through the origin.

Our next contribution was to understand classification in the KWIK framework so that the working of the classifier results in the agent taking near optimal actions if it does take an action autonomously. Here, we provided the groundwork an algorithm that works on a multi-dimensional space. We discussed insights that help us understand when the learner can be confident about its prediction and when it \textit{should} not be. 

We have thus provided the scaffolding for understanding KWIK Inverse Reinforcement Learning. This work leads us to two different problems that are interesting to us: the problem of understanding KWIK classification and the problem of KWIK IRL itself. We discuss possible future directions in the next section.