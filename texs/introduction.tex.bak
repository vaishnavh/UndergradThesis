\todo{Use citations}

\todo{Explain the stuff more properly}

In this section, we will introduce the reader to some specific and fundamental concepts of reinforcement learning and machine learning that will be required to motivate and understand the problem. The technical preliminaries are postponed to a later section. For the sake of the reader who might be completely new to the reinforcement learning set-up, mathematical jargon will be cautiously avoided in this section. \\

We will begin by discussing the full reinforcement learning framework in Section \ref{sec:rl}  following which we will the \textit{inverse} reinforcement learning problem in the context of imitation learning. Next we will describe the online learning framework of Knows-What-It-Knows (KWIK). Finally, we briefly describe the problem that forms the goal of this work. \\

\section{Reinforcement Learning}
\label{sec:rl}
The \textit{full} reinforcement learning (RL) problem, which is what concerns us in this work is a way of addressing \textbf{sequential decision making problems}. These problems 	 consist of two entities: an \textbf{environment} and an \textbf{agent}. At various time-steps during what is called an episode, the environment is found to be at different \textbf{states}. At each time-step, the agent performs a specific \textbf{action}. The action results in two consequences which are some form of feedback from the environment. First, the agent receives a \textbf{reward}. Second, the environment makes a \textbf{transition} to another state. The `problem' here is that the agent is required to maximize some form of cumulative reward called as the \textbf{return}. As an example, a robot navigating through  a maze is a sequential decision making problem: at various instances, the robot makes a move in a specific dirIection (the action), finds itself in various positions in the maze (the state), and receives a reward (punishment) from the maze if it reaches its goal  (hits the wall).\\

When an agent is thus introduced to an unknown environment it has to learn through experience what is  the `best' action to be taken at every state. The quality of the action can be understood to correspond to the amount of return that is expected by taking that  action. We will call this state-action mapping that is to be learned as the \textbf{optimal policy}. This process of learning will involve \textbf{exploration} - that enables to the agent to understand the environment better - and also \textbf{exploitation} that ensures that the agent makes best use of what it has learnt. Thus, when an agent explores and receives a high reward, the reward is a means of reinforcement which encourages the agent to believe that the steps it took recently are good and can to be exploited later. \\

Algorithms that solve this problem are broadly of two kinds. One class of algorithms maintain a model of the world by approximately estimating the stochastic feedback of the environment - these are called \textbf{model-based} algorithms. The other class of algorithms  are \textbf{model-free} in that they do not maintain any such model but only work with some sort of estimate of the `value' or the goodness of the actions that can be taken at every state. \\

With this basic idea of the reinforcement learning problem, we now look at imitation learning and then describe how inverse reinforcement learning is one way of solving it.

\section{Imitation Learning}
\label{sec:imit}

Recall that the goal of the reinforcement learning problem is to find  the `optimal policy' i.e., a mapping that tells us what is the best action to perform at a given state in a sequential decision making process. The imitation learning problem gives an interesting twist to this problem: we no longer have a notion of an optimal policy; instead we want to learn a policy that best mimics the behaviour of another agent whom we choose to call the \textbf{expert}.  \\

Why is this problem relevant? Sometimes, when we want an agent to learn to perform a behaviour, we are required to set-up an environment that provides a platform for it to learn - and this implies that we will have to tune the rewards of the environment in such a way the agent learns to do exactly what we want it to do. For example, if we were to make a helicopter robot to learn to do loops in the air, we must provide reinforcement in the form of accurate rewards during each of its trials so that  it will eventually converge to the correct way of performing a loop. Practically, this turns out to be a daunting task for the system designer!\\

Hence, imitation learning techniques are a form of automating this process: instead of designing the rewards associated with the environment and hence helping the agent learn what to do, we use expert \textbf{demonstrations} which is now the means of  transferring knowledge to the agent.  These demonstrations are basically trajectories of state action pairs in the environment. The language of rewards is completely absent in this knowledge transfer. Naturally, this problem is also called as \textbf{learning from demonstrations (LfD)} or \textbf{apprenticeship learning}. \\

Imitation learning, much like reinforcement learning itself, has been solved naturally in two ways. The \textbf{model-free} algorithms for imitation learning aim at blindly imitating the expert's action - the learner infers the expert's favorite action at a given state and simply copies that. On the other hand, \textbf{model-based} imitation learning algorithms take a less simpler path: these algorithms try to estimate a reward function on the environment! These algorithms assume that the expert behaves in such a way that it secretly optimizes some return connected to an unknown reward function of the environment. Thus, these algorithms primarily aim at inferring these rewards and then use some black-box reinforcement learning algorithm to find the policy that is optimal on these rewards. The process of inferring rewards that best explain the expert's behavior is known as inverse reinforcement learning, which is the subject of our discussion in the next section. \\





\section{Inverse Reinforcement Learning}
\label{sec:irl}
Recall that in the reinforcement learning problem, as the agent interacts with the environment it receives rewards that helps the agent discriminate between what sequence of state-action pairs is \textit{good} and what sequence is \textit{bad}. In inverse reinforcement learning, we no longer have immediate access to such rewards. Instead, we only have trajectories of an expert that is assumed to be optimal (or close to optimal) with respect an underlying reward system. The main goal of inverse reinforcement learning is to to understand what these rewards are the helps us tell that the expert's demonstrated behavior is better than any other possible behavior. We could later use these rewards to build our own policy that is close to that of the expert, which is what imitation learning aspires to do.\\

\subsection{Generalization in IRL}
\label{sec:generalization}
One powerful aspect of inverse reinforcement learning is that it helps us \textbf{generalize} from expert demonstrations. Often rewards are represented as a function of certain attributes of the state-action pair that it corresponds to. That is each state-action pair's reward is encoded as a vector of features that describe the various characteristics of taking that action on that state. Depending on the task at hand,  different weights are associated with each of these features and hence the reward corresponding to the state-action pair is a linear combination of these weights and the features of the pair. \\


In this setup, estimating the reward boils down to determining the parameters of this function that takes in these state-action pair attributes as input and produces the real valued reward. Thus, by estimating these parameters or the weights through demonstrations on a small part of the state-action space, we will be able to evaluate actions on states that have \textit{not been} visited by the expert. Essentially, we will be able to learn what to do when the agent is placed in a new state where the expert did not happen to demonstrate any action. We must also note another advantage of inverse reinforcement learning: the parameters of this reward function gives us a \textbf{succinct representation} of the demonstrated trajectories.\\

In the following section, we focus on a completely different topic called as \textit{Knows-What-It-Knows} learning which is a framework for online learning that we would like to use for analysing imitation learning.

\section{Knows-What-It-Knows (KWIK) Learning}
\label{sec:kwik}

KWIK learning is basically a framework provided for designing online learning algorithms for which certain guarantees can be provided. In machine learning, a learner tries to approximate a unknown function that has been chosen by the environment. The knowledge about this function is derived through sample points that form the learner's experience. These \textbf{sample points} are pairs of inputs and outputs where the outputs tend to be noisy. Furthermore, these points are drawn from an underlying \textbf{data distribution}. An \textit{offline} learner is given a dataset of points from which it is expected to learn an estimate of the function.\\


A common way of evaluating these functions is to ensure that for any underlying function, some event of failure of the algorithm has a low probability of occurring. To understand this better, let us call a \textbf{run} of the algorithm as the process of sampling some datapoints from the distribution and using the algorithm to produce an estimate. We evaluate the accuracy of the function as the expected error of this function on a point randomly drawn from the distribution. We define the \textbf{failure} of the algorithm  for this run to be equivalent to achieving an accuracy below some preset \textbf{accuracy threshold}. We would like that the event that this failure happens (which depends on how the sample points happen to be drawn), is below a specific \textbf{failure threshold}. If such guarantees can be given  for an algorithm, the algorithm is called as a \textbf{Probably Approximately Correct} algorithm.\\

KWIK is similarly a framework for the \textit{online learning} setup. In online learning, the agent now receives input points one after the other. However, it is required to predict the outputs as it receives the points and is evaluated based on these outputs. KWIK simply requires that the algorithm achieves accuracy whenever it produces an output. However, the algorithm is not required to produce an output all the time! It can, for finitely many times, output a \textit{don't know}, requesting the teacher for the output. Thus, the learning experience comes from these \textit{don't knows}. The event of failure is associated with the above condition not holding good for a run: either the algorithm makes infinitely many requests, or makes an error on a point it did not query.\\

The KWIK framework for online learning provides a useful platform for studying reinforcement learning algorithms that are naturally online - the agent is seeing states on after the other and does not have, at one go, access to a vast dataset of experiences. \\

We must also dwell on how the KWIK framework is also associated with \textbf{active learning}. In active learning, we bestow a lot of power to the learner. The learner has the right to choose points from a pool of points, as to whether she wants the output corresponding to those subset of points (every output asked adds to the cost!). Active learning is relevant when asking for the output on all input points is redundant and adds to the cost of obtaining labels. KWIK happens to incorporate this active form of learning in its own way. A KWIK algorithm is \textbf{self-aware} in that it decides to query for the teacher's output on a select set of points and it is also aware that it does not need the teacher's output on the remaining points. Thus, KWIK also provides a framework for studying active learning algorithms. We thus discuss in the following section how KWIK can be used to study learning from demonstrations via inverse reinforcement learning. \\


\section{KWIK Inverse Reinforcement Learning}
Recall that active learning algorithms are useful when datapoints that a learner is trained on consists of many redundant input-output pairs. Hence, it would have been wiser to choose a subset of points for which we need the teacher's output so that we can take advantage of this redundancy and reduce the cost of querying. Similarly, in learning from demonstrations, we could expect many demonstrations to be redundant in that the expert demonstrates a lot of trajectories in the same region of the environment. It really does not help to ask for more trajectories in this region because we have learned as much as we could here. However, if we were to accidentally enter an unseen region in the environment, what do we do? We could ask for a demonstration! Here lies the motivation for designing an online algorithm for learning from demonstrations. \\

However, we must note that we might not always want a demonstration from an unseen state - what if we have seen a very similar state in the environment earlier where the expert has provided many demonstrations already? It would be redundant to ask for a demonstration in this unseen state. Hence, we would need strong active learning algorithms which will be able to understand when it would be wise to query and when it would not be. \\

Assuming we do not make a query to the expert, and allow the learning agent to take an action she thinks is the best from what she has seen of the teacher until now, how do we know that the action taken is `good enough'? This is where we make use of the KWIK guarantee which tells us that on every instance that the learner makes a prediction, the learner is nearly accurate. Since this accuracy does not translate to meaningful terms in the model-free imitation learning algorithm, we have chosen to work with inverse reinforcement learning based imitation learning, where this accuracy translates to taking an action that is almost good with respect to its return. \\


\section{Rest of the document}
