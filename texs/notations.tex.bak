\chapter*{\centerline{NOTATIONS}}
\todo{Fill up}
\addcontentsline{toc}{chapter}{NOTATIONS}
\begin{singlespace}
\begin{tabbing}
xxxxxxxxxxxxxxxxxxxxx \= xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \kill
$M$ 	\> A Markov Decision Process \\
$\mathcal{S}$		\> State space of the MDP $M$  \\
$\mathcal{A}$	\> Action space of the MDP $M$ \\
$R(\cdot)$ \> Reward function of the MDP $M$ \\
$T(\cdot)$ \> Transition function of the MDP $M$ \\
$\gamma$ \> Discount factor \\
$k$ \> Dimensionality of the reward parametrization of MDP $M$ \\
$\vec{\theta}$ \> A variable weight parameter \\
$\vec{\theta}_E$ \> The true (expert) weight parameter \\
$\hat{\vec{\theta}}$ \> An estimate of the weight parameter \\
$\vec{x}$ \> A multi-dimensional point represented as a vector \\
$\pi(\cdot)$ \> A policy on MDP $M$ \\
$V^{\pi}_M, V^{\pi}$ \> Value function of policy $\pi$ on MDP $M$ \\
$Q^{\pi}_M, Q^{\pi}$ \> State-Action value function of policy $\pi$ on MDP $M$ \\
$\mathcal{D}$ \> Set of demonstrations on $M$ \\
$Pr(\cdot)$ \> Probability of occurrence of event \\
$\mathbb{I}(\cdot)$ \> Indicator function \\
$\mathbb{E}[\cdot]$ \> Expectation of a random variable \\
$z$ \> Observation provided by the teacher \\
$\text{SGN}(\cdot)$, $\text{SGN} [\cdot]$\> Signum function \\
$\vec{R}$ \> Rewards of MDP $M$ represented as a vector $\in \mathbb{R}^{|\mathcal{S}|}$ \\
$\vec{T}_{\pi}$ \> Transition matrix corresponding to policy $\pi$ on MDP $M$ \\
$\vec{V}^{\pi}$ \> Vector of values of states corresponding to $\pi$ on MDP $M$\\
$\vec{d}_{\pi}(\cdot)$ \> Stationary distribution of a policy  $\pi$ \\
$\mathcal{C}$ \> KWIK Classification Algorithm \\
\end{tabbing}
\end{singlespace}
\pagebreak
\clearpage
