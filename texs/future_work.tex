
There are two primary directions for future work. One of the primary focuses for future work would be to study algorithms for classification in the KWIK framework as defined above. We have provided the groundwork for the classification algorithm which we however understand to have exponential bounds. This could be possible because of the extremely relaxed assumptions on the noise of the teacher. Perhaps with a more restrictive assumption on the noise, that is still practically realizable, we might be able to devise algorithms with polynomial bounds.

The other direction for future work would deal with studying the KWIK IRL problem itself. For example, we would be interested in addressing the issue that the protocol currently asks pairwise queries to determine the near-optimal action. The teacher responds to these queries accordingly. However, ideally we would like the teacher to directly point out the best action. Thus, the algorithm should be adapted to a teacher who merely tells it what the best action is.

We would also like to study the equivalence between the KWIK classification condition and the $\epsilon-$optimal condition. The current equivalence is not strong in that the KWIK classification condition is more restrictive than the $\epsilon-$optimality in imitation learning.

One important question we would like to address is the fact that in the current problem setting we assume that we have access to the cost function for each decision ($\vec{\Phi}_Q$) directly. What would be more interesting is having access only to the short term reward functions ($\vec{\Phi}_R$) from which we are required to infer   $\vec{\Phi}_Q$ by gradually learning more about the policy. Remember that $\vec{\Phi}_Q$  is dependent not only on $\vec{\Phi}_R$ and the structure of the MDP but also on the policy. 

